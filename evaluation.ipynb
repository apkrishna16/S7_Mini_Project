{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os   \n",
    "from sacrebleu import corpus_bleu \n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer  \n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Excel file\n",
    "file_path = \"reference_response_rag.csv\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    RAGEvaluator class provides methods to evaluate generated text using various metrics \n",
    "    like BLEU, ROUGE, BERTScore, and Perplexity, leveraging GPT-2 for perplexity calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path=\"local_gpt2\"):\n",
    "        \"\"\"\n",
    "        Initializes the RAGEvaluator by loading the GPT-2 model and tokenizer.\n",
    "\n",
    "        Parameters:\n",
    "        - model_path (str): Path to the local GPT-2 model directory (default: \"local_gpt2\").\n",
    "\n",
    "        Output:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.gpt2_model, self.gpt2_tokenizer = self.load_gpt2_model(model_path)\n",
    "\n",
    "    def load_gpt2_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Loads the GPT-2 model and tokenizer. If the model doesn't exist locally, downloads it.\n",
    "\n",
    "        Parameters:\n",
    "        - model_path (str): Path to the local GPT-2 model directory.\n",
    "\n",
    "        Output:\n",
    "        - Tuple[model, tokenizer]: The GPT-2 model and tokenizer.\n",
    "        \"\"\"\n",
    "        if os.path.exists(model_path):\n",
    "            model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        else:\n",
    "            model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            model.save_pretrained(model_path)\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def evaluate_bleu_rouge(self, candidates, references):\n",
    "        \"\"\"\n",
    "        Evaluates BLEU and ROUGE-1 scores for a set of generated responses and references.\n",
    "\n",
    "        Parameters:\n",
    "        - candidates (list of str): Generated text responses.\n",
    "        - references (list of str): Ground truth reference texts.\n",
    "\n",
    "        Output:\n",
    "        - Tuple[float, float]: BLEU score and ROUGE-1 f-measure.\n",
    "        \"\"\"\n",
    "        bleu_score = corpus_bleu(candidates, [references]).score\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [scorer.score(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "        rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
    "        return bleu_score, rouge1\n",
    "\n",
    "    def evaluate_bert_score(self, candidates, references):\n",
    "        \"\"\"\n",
    "        Evaluates BERTScore (Precision, Recall, F1) for generated responses and references.\n",
    "\n",
    "        Parameters:\n",
    "        - candidates (list of str): Generated text responses.\n",
    "        - references (list of str): Ground truth reference texts.\n",
    "\n",
    "        Output:\n",
    "        - Tuple[float, float, float]: BERT Precision, Recall, and F1 scores.\n",
    "        \"\"\"\n",
    "        P, R, F1 = score(candidates, references, lang=\"en\", model_type='bert-base-multilingual-cased')\n",
    "        return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "    def evaluate_perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Evaluates the perplexity of the generated text using the GPT-2 model.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The text to evaluate perplexity for.\n",
    "\n",
    "        Output:\n",
    "        - float: Perplexity value.\n",
    "        \"\"\"\n",
    "        encodings = self.gpt2_tokenizer(text, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids']\n",
    "        labels = input_ids.clone()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.gpt2_model(input_ids, labels=labels)\n",
    "        return torch.exp(outputs.loss).item()\n",
    "\n",
    "    def evaluate_all(self, response, reference):\n",
    "        \"\"\"\n",
    "        Evaluates the generated response against the reference using all metrics (BLEU, ROUGE-1, \n",
    "        BERTScore, and Perplexity).\n",
    "\n",
    "        Parameters:\n",
    "        - response (str): The generated response text.\n",
    "        - reference (str): The ground truth reference text.\n",
    "\n",
    "        Output:\n",
    "        - dict: Dictionary containing evaluation scores for BLEU, ROUGE-1, BERTScore (P, R, F1), \n",
    "                and Perplexity.\n",
    "        \"\"\"\n",
    "        candidates = [response]\n",
    "        references = [reference]\n",
    "        bleu, rouge1 = self.evaluate_bleu_rouge(candidates, references)\n",
    "        bert_p, bert_r, bert_f1 = self.evaluate_bert_score(candidates, references)\n",
    "        perplexity = self.evaluate_perplexity(response)\n",
    "        return {\n",
    "            \"BLEU\": bleu,\n",
    "            \"ROUGE-1\": rouge1,\n",
    "            \"BERT P\": bert_p,\n",
    "            \"BERT R\": bert_r,\n",
    "            \"BERT F1\": bert_f1,\n",
    "            \"Perplexity\": perplexity,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_excel(file_path):\n",
    "    \"\"\"\n",
    "    Evaluates text responses and references from an Excel file using various metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the Excel file containing 'response' and 'reference' columns.\n",
    "\n",
    "    Output:\n",
    "    - dict: Dictionary containing average scores for BLEU, ROUGE-1, BERTScore (P, R, F1), \n",
    "            and Perplexity across all rows in the Excel file.\n",
    "    \"\"\"\n",
    "    evaluator = RAGEvaluator()\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    scores = []\n",
    "    for index, row in df.iterrows():\n",
    "        response = row['response']\n",
    "        reference = row['reference'] \n",
    "        scores.append(evaluator.evaluate_all(response, reference)) \n",
    " \n",
    "    avg_scores = {metric: sum(score[metric] for score in scores) / len(scores) for metric in scores[0].keys()}\n",
    "\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "BLEU: 14.9585\n",
      "ROUGE-1: 0.4522\n",
      "BERT P: 0.7491\n",
      "BERT R: 0.7438\n",
      "BERT F1: 0.7460\n",
      "Perplexity: 30.6252\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print average scores\n",
    "average_scores = evaluate_from_excel(file_path)\n",
    "print(\"Average Scores:\")\n",
    "for metric, value in average_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
